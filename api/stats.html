
<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta charset="utf-8" />
    <title>Stats &#8212; PyMC3 3.8 documentation</title>
    <link rel="stylesheet" href="../_static/semantic-sphinx.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/semantic-ui@2.4.2/dist/semantic.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/default.css" />
    <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script type="text/javascript" src="../_static/jquery.js"></script>
    <script type="text/javascript" src="../_static/underscore.js"></script>
    <script type="text/javascript" src="../_static/doctools.js"></script>
    <script type="text/javascript" src="../_static/language_data.js"></script>
    <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true, "ignoreClass": "document", "processClass": "math|output_area"}})</script>
    <script type="text/javascript" src="../_static/highlight.min.js"></script>
    <script type="text/javascript" src="../_static/semantic.min.js"></script>
    <link rel="shortcut icon" href="../_static/PyMC3.ico"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
<script>hljs.initHighlightingOnLoad();</script>
<meta charset='utf-8'>
<meta http-equiv='X-UA-Compatible' content='IE=edge,chrome=1'>
<meta name='viewport' content='width=device-width, initial-scale=1.0, maximum-scale=1'>
<meta name="apple-mobile-web-app-capable" content="yes">



  </head><body>
<div class="ui vertical center aligned">

    <div class="ui container">
        <div class="ui large secondary pointing menu">
            <a class="item" href="/">
                <img class="ui bottom aligned tiny image" src="https://cdn.rawgit.com/pymc-devs/pymc3/master/docs/logos/svg/PyMC3_banner.svg" />
            </a>
             <a href="../nb_tutorials/index.html" class="item">Tutorials</a> <a href="../nb_examples/index.html" class="item">Examples</a> <a href="../learn.html" class="item">Books + Videos</a> <a href="../api.html" class="item">API</a> <a href="../developer_guide.html" class="item">Developer Guide</a> <a href="../history.html" class="item">About PyMC3</a>
            
            <div class="right menu">
                <div class="item">
                    <form class="ui icon input" action="../search.html" method="get">
                        <input type="text" placeholder="Search..." name="q" />
                        <i class="search link icon"></i>
                    </form>
                </div>
                <a class="item" href="https://github.com/pymc-devs/pymc3"><i class="github blue icon large"></i></a>
            </div>
        </div>
    </div>
    
</div>

<div class="ui container" role="main">
    

    <div class="ui vertical segment">
        
  <div class="section" id="module-pymc3.stats">
<span id="stats"></span><h1>Stats<a class="headerlink" href="#module-pymc3.stats" title="Permalink to this headline">¶</a></h1>
<p>Statistical utility functions for PyMC3</p>
<p>Diagnostics and auxiliary statistical functions are delegated to the ArviZ library, a general
purpose library for “exploratory analysis of Bayesian models.” See
<a class="reference external" href="https://arviz-devs.github.io/arviz/">https://arviz-devs.github.io/arviz/</a> for details.</p>
<dl class="function">
<dt id="pymc3.stats.bfmi">
<code class="sig-prename descclassname">pymc3.stats.</code><code class="sig-name descname">bfmi</code><span class="sig-paren">(</span><em class="sig-param">data</em><span class="sig-paren">)</span><a class="headerlink" href="#pymc3.stats.bfmi" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculate the estimated Bayesian fraction of missing information (BFMI).</p>
<p>BFMI quantifies how well momentum resampling matches the marginal energy distribution. For more
information on BFMI, see <a class="reference external" href="https://arxiv.org/pdf/1604.00695v1.pdf">https://arxiv.org/pdf/1604.00695v1.pdf</a>. The current advice is that
values smaller than 0.3 indicate poor sampling. However, this threshold is provisional and may
change. See <a class="reference external" href="http://mc-stan.org/users/documentation/case-studies/pystan_workflow.html">http://mc-stan.org/users/documentation/case-studies/pystan_workflow.html</a> for more
information.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>data</strong><span class="classifier">obj</span></dt><dd><p>Any object that can be converted to an az.InferenceData object.
Refer to documentation of az.convert_to_dataset for details.
If InferenceData, energy variable needs to be found.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt><strong>z</strong><span class="classifier">array</span></dt><dd><p>The Bayesian fraction of missing information of the model and trace. One element per
chain in the trace.</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Examples</p>
<p>Compute the BFMI of an InferenceData object
.. ipython:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">In</span> <span class="p">[</span><span class="mi">1</span><span class="p">]:</span> <span class="kn">import</span> <span class="nn">arviz</span> <span class="k">as</span> <span class="nn">az</span>
   <span class="o">...</span><span class="p">:</span> <span class="n">data</span> <span class="o">=</span> <span class="n">az</span><span class="o">.</span><span class="n">load_arviz_data</span><span class="p">(</span><span class="s1">&#39;radon&#39;</span><span class="p">)</span>
   <span class="o">...</span><span class="p">:</span> <span class="n">az</span><span class="o">.</span><span class="n">bfmi</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="pymc3.stats.compare">
<code class="sig-prename descclassname">pymc3.stats.</code><code class="sig-name descname">compare</code><span class="sig-paren">(</span><em class="sig-param">dataset_dict</em>, <em class="sig-param">ic=None</em>, <em class="sig-param">method='BB-pseudo-BMA'</em>, <em class="sig-param">b_samples=1000</em>, <em class="sig-param">alpha=1</em>, <em class="sig-param">seed=None</em>, <em class="sig-param">scale='deviance'</em><span class="sig-paren">)</span><a class="headerlink" href="#pymc3.stats.compare" title="Permalink to this definition">¶</a></dt>
<dd><p>Compare models based on WAIC or LOO cross-validation.</p>
<p>WAIC is the widely applicable information criterion, and LOO is leave-one-out
(LOO) cross-validation. Read more theory here - in a paper by some of the
leading authorities on model selection - dx.doi.org/10.1111/1467-9868.00353</p>
<dl class="field-list">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl>
<dt><strong>dataset_dict</strong><span class="classifier">dict[str] -&gt; InferenceData</span></dt><dd><p>A dictionary of model names and InferenceData objects</p>
</dd>
<dt><strong>ic</strong><span class="classifier">str</span></dt><dd><p>Information Criterion (WAIC or LOO) used to compare models. Defaults to
<code class="docutils literal notranslate"><span class="pre">rcParams[&quot;stats.information_criterion&quot;]</span></code>.</p>
</dd>
<dt><strong>method</strong><span class="classifier">str</span></dt><dd><p>Method used to estimate the weights for each model. Available options are:</p>
<ul class="simple">
<li><p>‘stacking’ : stacking of predictive distributions.</p></li>
<li><p>‘BB-pseudo-BMA’ : (default) pseudo-Bayesian Model averaging using Akaike-type
weighting. The weights are stabilized using the Bayesian bootstrap.</p></li>
<li><p>‘pseudo-BMA’: pseudo-Bayesian Model averaging using Akaike-type
weighting, without Bootstrap stabilization (not recommended).</p></li>
</ul>
<p>For more information read <a class="reference external" href="https://arxiv.org/abs/1704.02030">https://arxiv.org/abs/1704.02030</a></p>
</dd>
<dt><strong>b_samples: int</strong></dt><dd><p>Number of samples taken by the Bayesian bootstrap estimation.
Only useful when method = ‘BB-pseudo-BMA’.</p>
</dd>
<dt><strong>alpha</strong><span class="classifier">float</span></dt><dd><p>The shape parameter in the Dirichlet distribution used for the Bayesian bootstrap. Only
useful when method = ‘BB-pseudo-BMA’. When alpha=1 (default), the distribution is uniform
on the simplex. A smaller alpha will keeps the final weights more away from 0 and 1.</p>
</dd>
<dt><strong>seed</strong><span class="classifier">int or np.random.RandomState instance</span></dt><dd><p>If int or RandomState, use it for seeding Bayesian bootstrap. Only
useful when method = ‘BB-pseudo-BMA’. Default None the global
np.random state is used.</p>
</dd>
<dt><strong>scale</strong><span class="classifier">str</span></dt><dd><p>Output scale for IC. Available options are:</p>
<ul class="simple">
<li><p><cite>deviance</cite> : (default) -2 * (log-score)</p></li>
<li><p><cite>log</cite> : 1 * log-score (after Vehtari et al. (2017))</p></li>
<li><p><cite>negative_log</cite> : -1 * (log-score)</p></li>
</ul>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt>A DataFrame, ordered from best to worst model (measured by information criteria).</dt><dd></dd>
<dt>The index reflects the key with which the models are passed to this function. The columns are:</dt><dd></dd>
<dt><strong>rank</strong><span class="classifier">The rank-order of the models. 0 is the best.</span></dt><dd></dd>
<dt><strong>IC</strong><span class="classifier">Information Criteria (WAIC or LOO).</span></dt><dd><p>Smaller IC indicates higher out-of-sample predictive fit (“better” model). Default WAIC.
If <cite>scale == log</cite> higher IC indicates higher out-of-sample predictive fit (“better” model).</p>
</dd>
<dt><strong>pIC</strong><span class="classifier">Estimated effective number of parameters.</span></dt><dd></dd>
<dt><strong>dIC</strong><span class="classifier">Relative difference between each IC (WAIC or LOO) and the lowest IC (WAIC or LOO).</span></dt><dd><p>It’s always 0 for the top-ranked model.</p>
</dd>
<dt>weight: Relative weight for each model.</dt><dd><p>This can be loosely interpreted as the probability of each model (among the compared model)
given the data. By default the uncertainty in the weights estimation is considered using
Bayesian bootstrap.</p>
</dd>
<dt><strong>SE</strong><span class="classifier">Standard error of the IC estimate.</span></dt><dd><p>If method = BB-pseudo-BMA these values are estimated using Bayesian bootstrap.</p>
</dd>
<dt><strong>dSE</strong><span class="classifier">Standard error of the difference in IC between each model and the top-ranked model.</span></dt><dd><p>It’s always 0 for the top-ranked model.</p>
</dd>
<dt><strong>warning</strong><span class="classifier">A value of 1 indicates that the computation of the IC may not be reliable.</span></dt><dd><p>This could be indication of WAIC/LOO starting to fail see
<a class="reference external" href="http://arxiv.org/abs/1507.04544">http://arxiv.org/abs/1507.04544</a> for details.</p>
</dd>
<dt><strong>scale</strong><span class="classifier">Scale used for the IC.</span></dt><dd></dd>
</dl>
</dd>
</dl>
<p class="rubric">Examples</p>
<p>Compare the centered and non centered models of the eight school problem:</p>
<div class="highlight-ipython notranslate"><div class="highlight"><pre><span></span><span class="gp">In [1]: </span><span class="kn">import</span> <span class="nn">arviz</span> <span class="k">as</span> <span class="nn">az</span>
<span class="gp">   ...: </span><span class="n">data1</span> <span class="o">=</span> <span class="n">az</span><span class="o">.</span><span class="n">load_arviz_data</span><span class="p">(</span><span class="s2">&quot;non_centered_eight&quot;</span><span class="p">)</span>
<span class="gp">   ...: </span><span class="n">data2</span> <span class="o">=</span> <span class="n">az</span><span class="o">.</span><span class="n">load_arviz_data</span><span class="p">(</span><span class="s2">&quot;centered_eight&quot;</span><span class="p">)</span>
<span class="gp">   ...: </span><span class="n">compare_dict</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;non centered&quot;</span><span class="p">:</span> <span class="n">data1</span><span class="p">,</span> <span class="s2">&quot;centered&quot;</span><span class="p">:</span> <span class="n">data2</span><span class="p">}</span>
<span class="gp">   ...: </span><span class="n">az</span><span class="o">.</span><span class="n">compare</span><span class="p">(</span><span class="n">compare_dict</span><span class="p">)</span>
<span class="gp">   ...: </span>
<span class="gh">Out[1]: </span><span class="go"></span>
<span class="go">             rank    waic    p_waic    d_waic    weight       se       dse warning waic_scale</span>
<span class="go">non centered    0  61.292  0.800621         0  0.528547   2.5329         0   False   deviance</span>
<span class="go">centered        1  61.516  0.901656  0.223908  0.471453  2.64374  0.155474   False   deviance</span>
</pre></div>
</div>
<p>Compare the models using LOO-CV, returning the IC in log scale and calculating the
weights using the stacking method.</p>
<div class="highlight-ipython notranslate"><div class="highlight"><pre><span></span><span class="gp">In [2]: </span><span class="n">az</span><span class="o">.</span><span class="n">compare</span><span class="p">(</span><span class="n">compare_dict</span><span class="p">,</span> <span class="n">ic</span><span class="o">=</span><span class="s2">&quot;loo&quot;</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s2">&quot;stacking&quot;</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="s2">&quot;log&quot;</span><span class="p">)</span>
<span class="gh">Out[2]: </span><span class="go"></span>
<span class="go">             rank      loo     p_loo     d_loo weight       se        dse warning loo_scale</span>
<span class="go">non centered    0 -30.6873  0.841888         0    0.5    1.365          0   False       log</span>
<span class="go">centered        1 -30.8104  0.954053  0.123084    0.5  1.42684  0.0860464   False       log</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="pymc3.stats.ess">
<code class="sig-prename descclassname">pymc3.stats.</code><code class="sig-name descname">ess</code><span class="sig-paren">(</span><em class="sig-param">data</em>, <em class="sig-param">*</em>, <em class="sig-param">var_names=None</em>, <em class="sig-param">method='bulk'</em>, <em class="sig-param">relative=False</em>, <em class="sig-param">prob=None</em><span class="sig-paren">)</span><a class="headerlink" href="#pymc3.stats.ess" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculate estimate of the effective sample size.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>data</strong><span class="classifier">obj</span></dt><dd><p>Any object that can be converted to an az.InferenceData object.
Refer to documentation of az.convert_to_dataset for details.
For ndarray: shape = (chain, draw).
For n-dimensional ndarray transform first to dataset with az.convert_to_dataset.</p>
</dd>
<dt><strong>var_names</strong><span class="classifier">list</span></dt><dd><p>Names of variables to include in the effective_sample_size_mean report</p>
</dd>
<dt><strong>method</strong><span class="classifier">str</span></dt><dd><p>Select ess method. Valid methods are:</p>
<ul class="simple">
<li><p>“bulk”</p></li>
<li><p>“tail”     # prob, optional</p></li>
<li><p>“quantile” # prob</p></li>
<li><p>“mean” (old ess)</p></li>
<li><p>“sd”</p></li>
<li><p>“median”</p></li>
<li><p>“mad” (mean absolute deviance)</p></li>
<li><p>“z_scale”</p></li>
<li><p>“folded”</p></li>
<li><p>“identity”</p></li>
</ul>
</dd>
<dt><strong>relative</strong><span class="classifier">bool</span></dt><dd><p>Return relative ess
<cite>ress = ess / n</cite></p>
</dd>
<dt><strong>prob</strong><span class="classifier">float, optional</span></dt><dd><p>probability value for “tail” and “quantile” ess functions.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt>xarray.Dataset</dt><dd><p>Return the effective sample size, <span class="math notranslate nohighlight">\(\hat{N}_{eff}\)</span></p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>The basic ess diagnostic is computed by:
.. math:: hat{N}_{eff} = frac{MN}{hat{tau}}
.. math:: hat{tau} = -1 + 2 sum_{t’=0}^K hat{P}_{t’}</p>
<p>where <span class="math notranslate nohighlight">\(M\)</span> is the number of chains, <span class="math notranslate nohighlight">\(N\)</span> the number of draws,
<span class="math notranslate nohighlight">\(\hat{\rho}_t\)</span> is the estimated _autocorrelation at lag <span class="math notranslate nohighlight">\(t\)</span>, and
<span class="math notranslate nohighlight">\(K\)</span> is the last integer for which <span class="math notranslate nohighlight">\(\hat{P}_{K} = \hat{\rho}_{2K} +
\hat{\rho}_{2K+1}\)</span> is still positive.</p>
<p>The current implementation is similar to Stan, which uses Geyer’s initial monotone sequence
criterion (Geyer, 1992; Geyer, 2011).</p>
<p class="rubric">References</p>
<ul class="simple">
<li><p>Vehtari et al. (2019) see <a class="reference external" href="https://arxiv.org/abs/1903.08008">https://arxiv.org/abs/1903.08008</a></p></li>
<li><p><a class="reference external" href="https://mc-stan.org/docs/2_18/reference-manual/effective-sample-size-section.html">https://mc-stan.org/docs/2_18/reference-manual/effective-sample-size-section.html</a>
Section 15.4.2</p></li>
<li><p>Gelman et al. BDA (2014) Formula 11.8</p></li>
</ul>
<p class="rubric">Examples</p>
<p>Calculate the effective_sample_size using the default arguments:</p>
<div class="highlight-ipython notranslate"><div class="highlight"><pre><span></span><span class="gp">In [1]: </span><span class="kn">import</span> <span class="nn">arviz</span> <span class="k">as</span> <span class="nn">az</span>
<span class="gp">   ...: </span><span class="n">data</span> <span class="o">=</span> <span class="n">az</span><span class="o">.</span><span class="n">load_arviz_data</span><span class="p">(</span><span class="s1">&#39;non_centered_eight&#39;</span><span class="p">)</span>
<span class="gp">   ...: </span><span class="n">az</span><span class="o">.</span><span class="n">ess</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
<span class="gp">   ...: </span>
<span class="gh">Out[1]: </span><span class="go"></span>
<span class="go">&lt;xarray.Dataset&gt;</span>
<span class="go">Dimensions:  (school: 8)</span>
<span class="go">Coordinates:</span>
<span class="go">  * school   (school) object &#39;Choate&#39; &#39;Deerfield&#39; ... &quot;St. Paul&#39;s&quot; &#39;Mt. Hermon&#39;</span>
<span class="go">Data variables:</span>
<span class="go">    mu       float64 2.353e+03</span>
<span class="go">    theta_t  (school) float64 2.215e+03 3.157e+03 ... 2.679e+03 2.522e+03</span>
<span class="go">    tau      float64 1.268e+03</span>
<span class="go">    theta    (school) float64 2.298e+03 2.432e+03 ... 2.173e+03 2.277e+03</span>
</pre></div>
</div>
<p>Calculate the ress of some of the variables</p>
<div class="highlight-ipython notranslate"><div class="highlight"><pre><span></span><span class="gp">In [2]: </span><span class="n">az</span><span class="o">.</span><span class="n">ess</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">relative</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">var_names</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;mu&quot;</span><span class="p">,</span> <span class="s2">&quot;theta_t&quot;</span><span class="p">])</span>
<span class="gh">Out[2]: </span><span class="go"></span>
<span class="go">&lt;xarray.Dataset&gt;</span>
<span class="go">Dimensions:  (school: 8)</span>
<span class="go">Coordinates:</span>
<span class="go">  * school   (school) object &#39;Choate&#39; &#39;Deerfield&#39; ... &quot;St. Paul&#39;s&quot; &#39;Mt. Hermon&#39;</span>
<span class="go">Data variables:</span>
<span class="go">    mu       float64 1.176</span>
<span class="go">    theta_t  (school) float64 1.107 1.579 1.464 1.258 1.157 1.275 1.339 1.261</span>
</pre></div>
</div>
<p>Calculate the ess using the “tail” method, leaving the <cite>prob</cite> argument at its default
value.</p>
<div class="highlight-ipython notranslate"><div class="highlight"><pre><span></span><span class="gp">In [3]: </span><span class="n">az</span><span class="o">.</span><span class="n">ess</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s2">&quot;tail&quot;</span><span class="p">)</span>
<span class="gh">Out[3]: </span><span class="go"></span>
<span class="go">&lt;xarray.Dataset&gt;</span>
<span class="go">Dimensions:  (school: 8)</span>
<span class="go">Coordinates:</span>
<span class="go">  * school   (school) object &#39;Choate&#39; &#39;Deerfield&#39; ... &quot;St. Paul&#39;s&quot; &#39;Mt. Hermon&#39;</span>
<span class="go">Data variables:</span>
<span class="go">    mu       float64 1.401e+03</span>
<span class="go">    theta_t  (school) float64 1.45e+03 1.514e+03 ... 1.207e+03 1.589e+03</span>
<span class="go">    tau      float64 900.0</span>
<span class="go">    theta    (school) float64 1.445e+03 1.506e+03 ... 1.433e+03 1.418e+03</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="pymc3.stats.geweke">
<code class="sig-prename descclassname">pymc3.stats.</code><code class="sig-name descname">geweke</code><span class="sig-paren">(</span><em class="sig-param">ary</em>, <em class="sig-param">first=0.1</em>, <em class="sig-param">last=0.5</em>, <em class="sig-param">intervals=20</em><span class="sig-paren">)</span><a class="headerlink" href="#pymc3.stats.geweke" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute z-scores for convergence diagnostics.</p>
<p>Compare the mean of the first % of series with the mean of the last % of series. x is divided
into a number of segments for which this difference is computed. If the series is converged,
this score should oscillate between -1 and 1.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>ary</strong><span class="classifier">1D array-like</span></dt><dd><p>The trace of some stochastic parameter.</p>
</dd>
<dt><strong>first</strong><span class="classifier">float</span></dt><dd><p>The fraction of series at the beginning of the trace.</p>
</dd>
<dt><strong>last</strong><span class="classifier">float</span></dt><dd><p>The fraction of series at the end to be compared with the section
at the beginning.</p>
</dd>
<dt><strong>intervals</strong><span class="classifier">int</span></dt><dd><p>The number of segments.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt><strong>scores</strong><span class="classifier">list [[]]</span></dt><dd><p>Return a list of [i, score], where i is the starting index for each interval and score the
Geweke score on the interval.</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>The Geweke score on some series x is computed by:</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[\frac{E[x_s] - E[x_e]}{\sqrt{V[x_s] + V[x_e]}}\]</div>
</div></blockquote>
<p>where <span class="math notranslate nohighlight">\(E\)</span> stands for the mean, <span class="math notranslate nohighlight">\(V\)</span> the variance,
<span class="math notranslate nohighlight">\(x_s\)</span> a section at the start of the series and
<span class="math notranslate nohighlight">\(x_e\)</span> a section at the end of the series.</p>
<p class="rubric">References</p>
<ul class="simple">
<li><p>Geweke (1992)</p></li>
</ul>
</dd></dl>

<dl class="function">
<dt id="pymc3.stats.hpd">
<code class="sig-prename descclassname">pymc3.stats.</code><code class="sig-name descname">hpd</code><span class="sig-paren">(</span><em class="sig-param">ary</em>, <em class="sig-param">credible_interval=0.94</em>, <em class="sig-param">circular=False</em><span class="sig-paren">)</span><a class="headerlink" href="#pymc3.stats.hpd" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculate highest posterior density (HPD) of array for given credible_interval.</p>
<p>The HPD is the minimum width Bayesian credible interval (BCI). This implementation works only
for unimodal distributions.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>x</strong><span class="classifier">Numpy array</span></dt><dd><p>An array containing posterior samples</p>
</dd>
<dt><strong>credible_interval</strong><span class="classifier">float, optional</span></dt><dd><p>Credible interval to compute. Defaults to 0.94.</p>
</dd>
<dt><strong>circular</strong><span class="classifier">bool, optional</span></dt><dd><p>Whether to compute the hpd taking into account <cite>x</cite> is a circular variable
(in the range [-np.pi, np.pi]) or not. Defaults to False (i.e non-circular variables).</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt>np.ndarray</dt><dd><p>lower and upper value of the interval.</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Examples</p>
<p>Calculate the hpd of a Normal random variable:</p>
<div class="highlight-ipython notranslate"><div class="highlight"><pre><span></span><span class="gp">In [1]: </span><span class="kn">import</span> <span class="nn">arviz</span> <span class="k">as</span> <span class="nn">az</span>
<span class="gp">   ...: </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">   ...: </span><span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="mi">2000</span><span class="p">)</span>
<span class="gp">   ...: </span><span class="n">az</span><span class="o">.</span><span class="n">hpd</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">credible_interval</span><span class="o">=.</span><span class="mi">68</span><span class="p">)</span>
<span class="gp">   ...: </span>
<span class="gh">Out[1]: </span><span class="go">array([-0.98683183,  0.98103368])</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="pymc3.stats.loo">
<code class="sig-prename descclassname">pymc3.stats.</code><code class="sig-name descname">loo</code><span class="sig-paren">(</span><em class="sig-param">data</em>, <em class="sig-param">pointwise=False</em>, <em class="sig-param">reff=None</em>, <em class="sig-param">scale='deviance'</em><span class="sig-paren">)</span><a class="headerlink" href="#pymc3.stats.loo" title="Permalink to this definition">¶</a></dt>
<dd><p>Pareto-smoothed importance sampling leave-one-out cross-validation.</p>
<p>Calculates leave-one-out (LOO) cross-validation for out of sample predictive model fit,
following Vehtari et al. (2017). Cross-validation is computed using Pareto-smoothed
importance sampling (PSIS).</p>
<dl class="field-list">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl>
<dt><strong>data</strong><span class="classifier">obj</span></dt><dd><p>Any object that can be converted to an az.InferenceData object. Refer to documentation
of az.convert_to_inference_data for details</p>
</dd>
<dt><strong>pointwise</strong><span class="classifier">bool, optional</span></dt><dd><p>if True the pointwise predictive accuracy will be returned. Defaults to False</p>
</dd>
<dt><strong>reff</strong><span class="classifier">float, optional</span></dt><dd><p>Relative MCMC efficiency, <cite>ess / n</cite> i.e. number of effective samples divided by
the number of actual samples. Computed from trace by default.</p>
</dd>
<dt><strong>scale</strong><span class="classifier">str</span></dt><dd><p>Output scale for loo. Available options are:</p>
<ul class="simple">
<li><p><cite>deviance</cite> : (default) -2 * (log-score)</p></li>
<li><p><cite>log</cite> : 1 * log-score (after Vehtari et al. (2017))</p></li>
<li><p><cite>negative_log</cite> : -1 * (log-score)</p></li>
</ul>
<p>A higher log-score (or a lower deviance) indicates a model with better predictive
accuracy.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt>pandas.Series with the following rows:</dt><dd></dd>
<dt><strong>loo</strong><span class="classifier">approximated Leave-one-out cross-validation</span></dt><dd></dd>
<dt><strong>loo_se</strong><span class="classifier">standard error of loo</span></dt><dd></dd>
<dt><strong>p_loo</strong><span class="classifier">effective number of parameters</span></dt><dd></dd>
<dt><strong>shape_warn</strong><span class="classifier">bool</span></dt><dd><p>True if the estimated shape parameter of
Pareto distribution is greater than 0.7 for one or more samples</p>
</dd>
<dt><strong>loo_i</strong><span class="classifier">array of pointwise predictive accuracy, only if pointwise True</span></dt><dd></dd>
<dt><strong>pareto_k</strong><span class="classifier">array of Pareto shape values, only if pointwise True</span></dt><dd></dd>
<dt><strong>loo_scale</strong><span class="classifier">scale of the loo results</span></dt><dd><p>The returned object has a custom print method that overrides pd.Series method. It is
specific to expected log pointwise predictive density (elpd) information criteria.</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Examples</p>
<p>Calculate the LOO-CV of a model:</p>
<div class="highlight-ipython notranslate"><div class="highlight"><pre><span></span><span class="gp">In [1]: </span><span class="kn">import</span> <span class="nn">arviz</span> <span class="k">as</span> <span class="nn">az</span>
<span class="gp">   ...: </span><span class="n">data</span> <span class="o">=</span> <span class="n">az</span><span class="o">.</span><span class="n">load_arviz_data</span><span class="p">(</span><span class="s2">&quot;centered_eight&quot;</span><span class="p">)</span>
<span class="gp">   ...: </span><span class="n">az</span><span class="o">.</span><span class="n">loo</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
<span class="gp">   ...: </span>
<span class="gh">Out[1]: </span><span class="go"></span>
<span class="go">Computed from 2000 by 8 log-likelihood matrix</span>

<span class="go">       Estimate       SE</span>
<span class="go">IC_loo    61.62     2.85</span>
<span class="go">p_loo      0.95        -</span>
</pre></div>
</div>
<p>The custom print method can be seen here, printing only the relevant information and
with a specific organization. <code class="docutils literal notranslate"><span class="pre">IC_loo</span></code> stands for information criteria, which is the
<cite>deviance</cite> scale, the <cite>log</cite> (and <cite>negative_log</cite>) correspond to <code class="docutils literal notranslate"><span class="pre">elpd</span></code> (and <code class="docutils literal notranslate"><span class="pre">-elpd</span></code>)</p>
<div class="highlight-ipython notranslate"><div class="highlight"><pre><span></span><span class="gp">In [2]: </span><span class="n">az</span><span class="o">.</span><span class="n">loo</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">pointwise</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="s2">&quot;log&quot;</span><span class="p">)</span>
<span class="gh">Out[2]: </span><span class="go"></span>
<span class="go">Computed from 2000 by 8 log-likelihood matrix</span>

<span class="go">         Estimate       SE</span>
<span class="go">elpd_loo   -30.81     1.43</span>
<span class="go">p_loo        0.95        -</span>
<span class="gt">------</span>

<span class="n">Pareto</span> <span class="n">k</span> <span class="n">diagnostic</span> <span class="n">values</span><span class="p">:</span>
                         <span class="n">Count</span>   <span class="n">Pct</span><span class="o">.</span>
<span class="p">(</span><span class="o">-</span><span class="n">Inf</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">]</span>   <span class="p">(</span><span class="n">good</span><span class="p">)</span>        <span class="mi">6</span>   <span class="mf">75.0</span><span class="o">%</span>
 <span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.7</span><span class="p">]</span>   <span class="p">(</span><span class="n">ok</span><span class="p">)</span>          <span class="mi">2</span>   <span class="mf">25.0</span><span class="o">%</span>
   <span class="p">(</span><span class="mf">0.7</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>   <span class="p">(</span><span class="n">bad</span><span class="p">)</span>         <span class="mi">0</span>    <span class="mf">0.0</span><span class="o">%</span>
   <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">Inf</span><span class="p">)</span>   <span class="p">(</span><span class="n">very</span> <span class="n">bad</span><span class="p">)</span>    <span class="mi">0</span>    <span class="mf">0.0</span><span class="o">%</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="pymc3.stats.mcse">
<code class="sig-prename descclassname">pymc3.stats.</code><code class="sig-name descname">mcse</code><span class="sig-paren">(</span><em class="sig-param">data</em>, <em class="sig-param">*</em>, <em class="sig-param">var_names=None</em>, <em class="sig-param">method='mean'</em>, <em class="sig-param">prob=None</em><span class="sig-paren">)</span><a class="headerlink" href="#pymc3.stats.mcse" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculate Markov Chain Standard Error statistic.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>data</strong><span class="classifier">obj</span></dt><dd><p>Any object that can be converted to an az.InferenceData object
Refer to documentation of az.convert_to_dataset for details
For ndarray: shape = (chain, draw).
For n-dimensional ndarray transform first to dataset with az.convert_to_dataset.</p>
</dd>
<dt><strong>var_names</strong><span class="classifier">list</span></dt><dd><p>Names of variables to include in the rhat report</p>
</dd>
<dt><strong>method</strong><span class="classifier">str</span></dt><dd><p>Select mcse method. Valid methods are:
- “mean”
- “sd”
- “quantile”</p>
</dd>
<dt><strong>prob</strong><span class="classifier">float</span></dt><dd><p>Quantile information.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt>xarray.Dataset</dt><dd><p>Return the msce dataset</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Examples</p>
<p>Calculate the Markov Chain Standard Error using the default arguments:</p>
<div class="highlight-ipython notranslate"><div class="highlight"><pre><span></span><span class="gp">In [1]: </span><span class="kn">import</span> <span class="nn">arviz</span> <span class="k">as</span> <span class="nn">az</span>
<span class="gp">   ...: </span><span class="n">data</span> <span class="o">=</span> <span class="n">az</span><span class="o">.</span><span class="n">load_arviz_data</span><span class="p">(</span><span class="s2">&quot;non_centered_eight&quot;</span><span class="p">)</span>
<span class="gp">   ...: </span><span class="n">az</span><span class="o">.</span><span class="n">mcse</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
<span class="gp">   ...: </span>
<span class="gh">Out[1]: </span><span class="go"></span>
<span class="go">&lt;xarray.Dataset&gt;</span>
<span class="go">Dimensions:  (school: 8)</span>
<span class="go">Coordinates:</span>
<span class="go">  * school   (school) object &#39;Choate&#39; &#39;Deerfield&#39; ... &quot;St. Paul&#39;s&quot; &#39;Mt. Hermon&#39;</span>
<span class="go">Data variables:</span>
<span class="go">    mu       float64 0.06787</span>
<span class="go">    theta_t  (school) float64 0.02117 0.01655 0.01758 ... 0.01886 0.0185 0.01861</span>
<span class="go">    tau      float64 0.0739</span>
<span class="go">    theta    (school) float64 0.1196 0.09312 0.1104 ... 0.09868 0.1054 0.1068</span>
</pre></div>
</div>
<p>Calculate the Markov Chain Standard Error using the quantile method:</p>
<div class="highlight-ipython notranslate"><div class="highlight"><pre><span></span><span class="gp">In [2]: </span><span class="n">az</span><span class="o">.</span><span class="n">mcse</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s2">&quot;quantile&quot;</span><span class="p">,</span> <span class="n">prob</span><span class="o">=.</span><span class="mi">7</span><span class="p">)</span>
<span class="gh">Out[2]: </span><span class="go"></span>
<span class="go">&lt;xarray.Dataset&gt;</span>
<span class="go">Dimensions:  (school: 8)</span>
<span class="go">Coordinates:</span>
<span class="go">  * school   (school) object &#39;Choate&#39; &#39;Deerfield&#39; ... &quot;St. Paul&#39;s&quot; &#39;Mt. Hermon&#39;</span>
<span class="go">Data variables:</span>
<span class="go">    mu       float64 0.0966</span>
<span class="go">    theta_t  (school) float64 0.02069 0.03194 0.02927 ... 0.02107 0.02851</span>
<span class="go">    tau      float64 0.08466</span>
<span class="go">    theta    (school) float64 0.1748 0.1347 0.1245 ... 0.1247 0.1112 0.1243</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="pymc3.stats.r2_score">
<code class="sig-prename descclassname">pymc3.stats.</code><code class="sig-name descname">r2_score</code><span class="sig-paren">(</span><em class="sig-param">y_true</em>, <em class="sig-param">y_pred</em><span class="sig-paren">)</span><a class="headerlink" href="#pymc3.stats.r2_score" title="Permalink to this definition">¶</a></dt>
<dd><p>R² for Bayesian regression models. Only valid for linear models.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>y_true</strong><span class="classifier">array-like of shape = (n_samples) or (n_samples, n_outputs)</span></dt><dd><p>Ground truth (correct) target values.</p>
</dd>
<dt><strong>y_pred</strong><span class="classifier">array-like of shape = (n_samples) or (n_samples, n_outputs)</span></dt><dd><p>Estimated target values.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt>Pandas Series with the following indices:</dt><dd></dd>
<dt>r2: Bayesian R²</dt><dd></dd>
<dt>r2_std: standard deviation of the Bayesian R².</dt><dd></dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="pymc3.stats.rhat">
<code class="sig-prename descclassname">pymc3.stats.</code><code class="sig-name descname">rhat</code><span class="sig-paren">(</span><em class="sig-param">data</em>, <em class="sig-param">*</em>, <em class="sig-param">var_names=None</em>, <em class="sig-param">method='rank'</em><span class="sig-paren">)</span><a class="headerlink" href="#pymc3.stats.rhat" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute estimate of rank normalized splitR-hat for a set of traces.</p>
<p>The rank normalized R-hat diagnostic tests for lack of convergence by comparing the variance
between multiple chains to the variance within each chain. If convergence has been achieved,
the between-chain and within-chain variances should be identical. To be most effective in
detecting evidence for nonconvergence, each chain should have been initialized to starting
values that are dispersed relative to the target distribution.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>data</strong><span class="classifier">obj</span></dt><dd><p>Any object that can be converted to an az.InferenceData object.
Refer to documentation of az.convert_to_dataset for details.
At least 2 posterior chains are needed to compute this diagnostic of one or more
stochastic parameters.
For ndarray: shape = (chain, draw).
For n-dimensional ndarray transform first to dataset with az.convert_to_dataset.</p>
</dd>
<dt><strong>var_names</strong><span class="classifier">list</span></dt><dd><p>Names of variables to include in the rhat report</p>
</dd>
<dt><strong>method</strong><span class="classifier">str</span></dt><dd><p>Select R-hat method. Valid methods are:
- “rank”        # recommended by Vehtari et al. (2019)
- “split”
- “folded”
- “z_scale”
- “identity”</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt>xarray.Dataset</dt><dd><p>Returns dataset of the potential scale reduction factors, <span class="math notranslate nohighlight">\(\hat{R}\)</span></p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>The diagnostic is computed by:</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[\hat{R} = \frac{\hat{V}}{W}\]</div>
</div></blockquote>
<p>where <span class="math notranslate nohighlight">\(W\)</span> is the within-chain variance and <span class="math notranslate nohighlight">\(\hat{V}\)</span> is the posterior variance
estimate for the pooled rank-traces. This is the potential scale reduction factor, which
converges to unity when each of the traces is a sample from the target posterior. Values
greater than one indicate that one or more chains have not yet converged.</p>
<p>Rank values are calculated over all the chains with <cite>scipy.stats.rankdata</cite>.
Each chain is split in two and normalized with the z-transform following Vehtari et al. (2019).</p>
<p class="rubric">References</p>
<ul class="simple">
<li><p>Vehtari et al. (2019) see <a class="reference external" href="https://arxiv.org/abs/1903.08008">https://arxiv.org/abs/1903.08008</a></p></li>
<li><p>Gelman et al. BDA (2014)</p></li>
<li><p>Brooks and Gelman (1998)</p></li>
<li><p>Gelman and Rubin (1992)</p></li>
</ul>
<p class="rubric">Examples</p>
<p>Calculate the R-hat using the default arguments:</p>
<div class="highlight-ipython notranslate"><div class="highlight"><pre><span></span><span class="gp">In [1]: </span><span class="kn">import</span> <span class="nn">arviz</span> <span class="k">as</span> <span class="nn">az</span>
<span class="gp">   ...: </span><span class="n">data</span> <span class="o">=</span> <span class="n">az</span><span class="o">.</span><span class="n">load_arviz_data</span><span class="p">(</span><span class="s2">&quot;non_centered_eight&quot;</span><span class="p">)</span>
<span class="gp">   ...: </span><span class="n">az</span><span class="o">.</span><span class="n">rhat</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
<span class="gp">   ...: </span>
<span class="gh">Out[1]: </span><span class="go"></span>
<span class="go">&lt;xarray.Dataset&gt;</span>
<span class="go">Dimensions:  (school: 8)</span>
<span class="go">Coordinates:</span>
<span class="go">  * school   (school) object &#39;Choate&#39; &#39;Deerfield&#39; ... &quot;St. Paul&#39;s&quot; &#39;Mt. Hermon&#39;</span>
<span class="go">Data variables:</span>
<span class="go">    mu       float64 1.0</span>
<span class="go">    theta_t  (school) float64 1.001 1.002 1.004 1.0 0.9999 1.0 1.0 1.006</span>
<span class="go">    tau      float64 1.001</span>
<span class="go">    theta    (school) float64 1.001 1.001 1.008 1.001 1.0 1.002 1.001 1.001</span>
</pre></div>
</div>
<p>Calculate the R-hat of some variables using the folded method:</p>
<div class="highlight-ipython notranslate"><div class="highlight"><pre><span></span><span class="gp">In [2]: </span><span class="n">az</span><span class="o">.</span><span class="n">rhat</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">var_names</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;mu&quot;</span><span class="p">,</span> <span class="s2">&quot;theta_t&quot;</span><span class="p">],</span> <span class="n">method</span><span class="o">=</span><span class="s2">&quot;folded&quot;</span><span class="p">)</span>
<span class="gh">Out[2]: </span><span class="go"></span>
<span class="go">&lt;xarray.Dataset&gt;</span>
<span class="go">Dimensions:  (school: 8)</span>
<span class="go">Coordinates:</span>
<span class="go">  * school   (school) object &#39;Choate&#39; &#39;Deerfield&#39; ... &quot;St. Paul&#39;s&quot; &#39;Mt. Hermon&#39;</span>
<span class="go">Data variables:</span>
<span class="go">    mu       float64 1.0</span>
<span class="go">    theta_t  (school) float64 1.0 1.002 1.004 1.0 0.9999 1.0 1.0 1.006</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="pymc3.stats.summary">
<code class="sig-prename descclassname">pymc3.stats.</code><code class="sig-name descname">summary</code><span class="sig-paren">(</span><em class="sig-param">data</em>, <em class="sig-param">var_names=None</em>, <em class="sig-param">fmt='wide'</em>, <em class="sig-param">round_to=None</em>, <em class="sig-param">include_circ=None</em>, <em class="sig-param">stat_funcs=None</em>, <em class="sig-param">extend=True</em>, <em class="sig-param">credible_interval=0.94</em>, <em class="sig-param">order='C'</em>, <em class="sig-param">index_origin=0</em><span class="sig-paren">)</span><a class="headerlink" href="#pymc3.stats.summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a data frame with summary statistics.</p>
<dl class="field-list">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl>
<dt><strong>data</strong><span class="classifier">obj</span></dt><dd><p>Any object that can be converted to an az.InferenceData object
Refer to documentation of az.convert_to_dataset for details</p>
</dd>
<dt><strong>var_names</strong><span class="classifier">list</span></dt><dd><p>Names of variables to include in summary</p>
</dd>
<dt><strong>include_circ</strong><span class="classifier">bool</span></dt><dd><p>Whether to include circular statistics</p>
</dd>
<dt><strong>fmt</strong><span class="classifier">{‘wide’, ‘long’, ‘xarray’}</span></dt><dd><p>Return format is either pandas.DataFrame {‘wide’, ‘long’} or xarray.Dataset {‘xarray’}.</p>
</dd>
<dt><strong>round_to</strong><span class="classifier">int</span></dt><dd><p>Number of decimals used to round results. Defaults to 2. Use “none” to return raw numbers.</p>
</dd>
<dt><strong>stat_funcs</strong><span class="classifier">dict</span></dt><dd><p>A list of functions or a dict of functions with function names as keys used to calculate
statistics. By default, the mean, standard deviation, simulation standard error, and
highest posterior density intervals are included.</p>
<p>The functions will be given one argument, the samples for a variable as an nD array,
The functions should be in the style of a ufunc and return a single number. For example,
<cite>np.mean</cite>, or <cite>scipy.stats.var</cite> would both work.</p>
</dd>
<dt><strong>extend</strong><span class="classifier">boolean</span></dt><dd><p>If True, use the statistics returned by <cite>stat_funcs</cite> in addition to, rather than in place
of, the default statistics. This is only meaningful when <cite>stat_funcs</cite> is not None.</p>
</dd>
<dt><strong>credible_interval</strong><span class="classifier">float, optional</span></dt><dd><p>Credible interval to plot. Defaults to 0.94. This is only meaningful when <cite>stat_funcs</cite> is
None.</p>
</dd>
<dt><strong>order</strong><span class="classifier">{“C”, “F”}</span></dt><dd><p>If fmt is “wide”, use either C or F unpacking order. Defaults to C.</p>
</dd>
<dt><strong>index_origin</strong><span class="classifier">int</span></dt><dd><p>If fmt is “wide, select n-based indexing for multivariate parameters. Defaults to 0.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt>pandas.DataFrame</dt><dd><p>With summary statistics for each variable. Defaults statistics are: <cite>mean</cite>, <cite>sd</cite>,
<cite>hpd_3%</cite>, <cite>hpd_97%</cite>, <cite>mcse_mean</cite>, <cite>mcse_sd</cite>, <cite>ess_bulk</cite>, <cite>ess_tail</cite> and <cite>r_hat</cite>.
<cite>r_hat</cite> is only computed for traces with 2 or more chains.</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="highlight-ipython notranslate"><div class="highlight"><pre><span></span><span class="gp">In [1]: </span><span class="kn">import</span> <span class="nn">arviz</span> <span class="k">as</span> <span class="nn">az</span>
<span class="gp">   ...: </span><span class="n">data</span> <span class="o">=</span> <span class="n">az</span><span class="o">.</span><span class="n">load_arviz_data</span><span class="p">(</span><span class="s2">&quot;centered_eight&quot;</span><span class="p">)</span>
<span class="gp">   ...: </span><span class="n">az</span><span class="o">.</span><span class="n">summary</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">var_names</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;mu&quot;</span><span class="p">,</span> <span class="s2">&quot;tau&quot;</span><span class="p">])</span>
<span class="gp">   ...: </span>
<span class="gh">Out[1]: </span><span class="go"></span>
<span class="go">      mean     sd  hpd_3%  hpd_97%  mcse_mean  mcse_sd  ess_mean  ess_sd  ess_bulk  ess_tail  r_hat</span>
<span class="go">mu   4.093  3.372  -2.118   10.403      0.215    0.152     246.0   246.0     251.0     643.0   1.03</span>
<span class="go">tau  4.089  3.001   0.569    9.386      0.252    0.178     142.0   142.0      79.0      54.0   1.07</span>
</pre></div>
</div>
<p>Other statistics can be calculated by passing a list of functions
or a dictionary with key, function pairs.</p>
<div class="highlight-ipython notranslate"><div class="highlight"><pre><span></span><span class="gp">In [2]: </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">   ...: </span><span class="k">def</span> <span class="nf">median_sd</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="gp">   ...: </span>    <span class="n">median</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">percentile</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">50</span><span class="p">)</span>
<span class="gp">   ...: </span>    <span class="n">sd</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">((</span><span class="n">x</span><span class="o">-</span><span class="n">median</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">))</span>
<span class="gp">   ...: </span>    <span class="k">return</span> <span class="n">sd</span>
<span class="gp">   ...: </span>
<span class="gp">   ...: </span><span class="n">func_dict</span> <span class="o">=</span> <span class="p">{</span>
<span class="gp">   ...: </span>    <span class="s2">&quot;std&quot;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">,</span>
<span class="gp">   ...: </span>    <span class="s2">&quot;median_std&quot;</span><span class="p">:</span> <span class="n">median_sd</span><span class="p">,</span>
<span class="gp">   ...: </span>    <span class="s2">&quot;5%&quot;</span><span class="p">:</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">percentile</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span>
<span class="gp">   ...: </span>    <span class="s2">&quot;median&quot;</span><span class="p">:</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">percentile</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">50</span><span class="p">),</span>
<span class="gp">   ...: </span>    <span class="s2">&quot;95%&quot;</span><span class="p">:</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">percentile</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">95</span><span class="p">),</span>
<span class="gp">   ...: </span><span class="p">}</span>
<span class="gp">   ...: </span><span class="n">az</span><span class="o">.</span><span class="n">summary</span><span class="p">(</span>
<span class="gp">   ...: </span>    <span class="n">data</span><span class="p">,</span>
<span class="gp">   ...: </span>    <span class="n">var_names</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;mu&quot;</span><span class="p">,</span> <span class="s2">&quot;tau&quot;</span><span class="p">],</span>
<span class="gp">   ...: </span>    <span class="n">stat_funcs</span><span class="o">=</span><span class="n">func_dict</span><span class="p">,</span>
<span class="gp">   ...: </span>    <span class="n">extend</span><span class="o">=</span><span class="kc">False</span>
<span class="gp">   ...: </span><span class="p">)</span>
<span class="gp">   ...: </span>
<span class="gh">Out[2]: </span><span class="go"></span>
<span class="go">       std  median_std     5%  median    95%</span>
<span class="go">mu   3.371       3.374 -1.312   3.961  9.640</span>
<span class="go">tau  3.000       3.113  0.785   3.258  9.659</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="pymc3.stats.waic">
<code class="sig-prename descclassname">pymc3.stats.</code><code class="sig-name descname">waic</code><span class="sig-paren">(</span><em class="sig-param">data</em>, <em class="sig-param">pointwise=False</em>, <em class="sig-param">scale='deviance'</em><span class="sig-paren">)</span><a class="headerlink" href="#pymc3.stats.waic" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculate the widely available information criterion.</p>
<p>Also calculates the WAIC’s standard error and the effective number of
parameters of the samples in trace from model. Read more theory here - in
a paper by some of the leading authorities on model selection
&lt;dx.doi.org/10.1111/1467-9868.00353&gt;</p>
<dl class="field-list">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl>
<dt><strong>data</strong><span class="classifier">obj</span></dt><dd><p>Any object that can be converted to an az.InferenceData object
Refer to documentation of az.convert_to_inference_data for details</p>
</dd>
<dt><strong>pointwise</strong><span class="classifier">bool</span></dt><dd><p>if True the pointwise predictive accuracy will be returned.
Default False</p>
</dd>
<dt><strong>scale</strong><span class="classifier">str</span></dt><dd><p>Output scale for loo. Available options are:</p>
<ul class="simple">
<li><p><cite>deviance</cite> : (default) -2 * (log-score)</p></li>
<li><p><cite>log</cite> : 1 * log-score</p></li>
<li><p><cite>negative_log</cite> : -1 * (log-score)</p></li>
</ul>
<p>A higher log-score (or a lower deviance) indicates a model with better predictive
accuracy.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt>Series with the following rows:</dt><dd></dd>
<dt><strong>waic</strong><span class="classifier">widely available information criterion</span></dt><dd></dd>
<dt><strong>waic_se</strong><span class="classifier">standard error of waic</span></dt><dd></dd>
<dt><strong>p_waic</strong><span class="classifier">effective number parameters</span></dt><dd></dd>
<dt><strong>var_warn</strong><span class="classifier">bool</span></dt><dd><p>True if posterior variance of the log predictive
densities exceeds 0.4</p>
</dd>
<dt><strong>waic_i</strong><span class="classifier">and array of the pointwise predictive accuracy, only if pointwise True</span></dt><dd></dd>
<dt><strong>waic_scale</strong><span class="classifier">scale of the waic results</span></dt><dd><p>The returned object has a custom print method that overrides pd.Series method. It is
specific to expected log pointwise predictive density (elpd) information criteria.</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Examples</p>
<p>Calculate the WAIC of a model:</p>
<div class="highlight-ipython notranslate"><div class="highlight"><pre><span></span><span class="gp">In [1]: </span><span class="kn">import</span> <span class="nn">arviz</span> <span class="k">as</span> <span class="nn">az</span>
<span class="gp">   ...: </span><span class="n">data</span> <span class="o">=</span> <span class="n">az</span><span class="o">.</span><span class="n">load_arviz_data</span><span class="p">(</span><span class="s2">&quot;centered_eight&quot;</span><span class="p">)</span>
<span class="gp">   ...: </span><span class="n">az</span><span class="o">.</span><span class="n">waic</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">pointwise</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">   ...: </span>
<span class="gh">Out[1]: </span><span class="go"></span>
<span class="go">Computed from 2000 by 8 log-likelihood matrix</span>

<span class="go">        Estimate       SE</span>
<span class="go">IC_waic    61.52     2.83</span>
<span class="go">p_waic      0.90        -</span>
</pre></div>
</div>
<p>The custom print method can be seen here, printing only the relevant information and
with a specific organization. <code class="docutils literal notranslate"><span class="pre">IC_loo</span></code> stands for information criteria, which is the
<cite>deviance</cite> scale, the <cite>log</cite> (and <cite>negative_log</cite>) correspond to <code class="docutils literal notranslate"><span class="pre">elpd</span></code> (and <code class="docutils literal notranslate"><span class="pre">-elpd</span></code>)</p>
</dd></dl>

<dl class="function">
<dt id="pymc3.stats.gelman_rubin">
<code class="sig-prename descclassname">pymc3.stats.</code><code class="sig-name descname">gelman_rubin</code><span class="sig-paren">(</span><em class="sig-param">*args</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#pymc3.stats.gelman_rubin" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute estimate of rank normalized splitR-hat for a set of traces.</p>
<p>The rank normalized R-hat diagnostic tests for lack of convergence by comparing the variance
between multiple chains to the variance within each chain. If convergence has been achieved,
the between-chain and within-chain variances should be identical. To be most effective in
detecting evidence for nonconvergence, each chain should have been initialized to starting
values that are dispersed relative to the target distribution.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>data</strong><span class="classifier">obj</span></dt><dd><p>Any object that can be converted to an az.InferenceData object.
Refer to documentation of az.convert_to_dataset for details.
At least 2 posterior chains are needed to compute this diagnostic of one or more
stochastic parameters.
For ndarray: shape = (chain, draw).
For n-dimensional ndarray transform first to dataset with az.convert_to_dataset.</p>
</dd>
<dt><strong>var_names</strong><span class="classifier">list</span></dt><dd><p>Names of variables to include in the rhat report</p>
</dd>
<dt><strong>method</strong><span class="classifier">str</span></dt><dd><p>Select R-hat method. Valid methods are:
- “rank”        # recommended by Vehtari et al. (2019)
- “split”
- “folded”
- “z_scale”
- “identity”</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt>xarray.Dataset</dt><dd><p>Returns dataset of the potential scale reduction factors, <span class="math notranslate nohighlight">\(\hat{R}\)</span></p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>The diagnostic is computed by:</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[\hat{R} = \frac{\hat{V}}{W}\]</div>
</div></blockquote>
<p>where <span class="math notranslate nohighlight">\(W\)</span> is the within-chain variance and <span class="math notranslate nohighlight">\(\hat{V}\)</span> is the posterior variance
estimate for the pooled rank-traces. This is the potential scale reduction factor, which
converges to unity when each of the traces is a sample from the target posterior. Values
greater than one indicate that one or more chains have not yet converged.</p>
<p>Rank values are calculated over all the chains with <cite>scipy.stats.rankdata</cite>.
Each chain is split in two and normalized with the z-transform following Vehtari et al. (2019).</p>
<p class="rubric">References</p>
<ul class="simple">
<li><p>Vehtari et al. (2019) see <a class="reference external" href="https://arxiv.org/abs/1903.08008">https://arxiv.org/abs/1903.08008</a></p></li>
<li><p>Gelman et al. BDA (2014)</p></li>
<li><p>Brooks and Gelman (1998)</p></li>
<li><p>Gelman and Rubin (1992)</p></li>
</ul>
<p class="rubric">Examples</p>
<p>Calculate the R-hat using the default arguments:</p>
<div class="highlight-ipython notranslate"><div class="highlight"><pre><span></span><span class="gp">In [1]: </span><span class="kn">import</span> <span class="nn">arviz</span> <span class="k">as</span> <span class="nn">az</span>
<span class="gp">   ...: </span><span class="n">data</span> <span class="o">=</span> <span class="n">az</span><span class="o">.</span><span class="n">load_arviz_data</span><span class="p">(</span><span class="s2">&quot;non_centered_eight&quot;</span><span class="p">)</span>
<span class="gp">   ...: </span><span class="n">az</span><span class="o">.</span><span class="n">rhat</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
<span class="gp">   ...: </span>
<span class="gh">Out[1]: </span><span class="go"></span>
<span class="go">&lt;xarray.Dataset&gt;</span>
<span class="go">Dimensions:  (school: 8)</span>
<span class="go">Coordinates:</span>
<span class="go">  * school   (school) object &#39;Choate&#39; &#39;Deerfield&#39; ... &quot;St. Paul&#39;s&quot; &#39;Mt. Hermon&#39;</span>
<span class="go">Data variables:</span>
<span class="go">    mu       float64 1.0</span>
<span class="go">    theta_t  (school) float64 1.001 1.002 1.004 1.0 0.9999 1.0 1.0 1.006</span>
<span class="go">    tau      float64 1.001</span>
<span class="go">    theta    (school) float64 1.001 1.001 1.008 1.001 1.0 1.002 1.001 1.001</span>
</pre></div>
</div>
<p>Calculate the R-hat of some variables using the folded method:</p>
<div class="highlight-ipython notranslate"><div class="highlight"><pre><span></span><span class="gp">In [2]: </span><span class="n">az</span><span class="o">.</span><span class="n">rhat</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">var_names</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;mu&quot;</span><span class="p">,</span> <span class="s2">&quot;theta_t&quot;</span><span class="p">],</span> <span class="n">method</span><span class="o">=</span><span class="s2">&quot;folded&quot;</span><span class="p">)</span>
<span class="gh">Out[2]: </span><span class="go"></span>
<span class="go">&lt;xarray.Dataset&gt;</span>
<span class="go">Dimensions:  (school: 8)</span>
<span class="go">Coordinates:</span>
<span class="go">  * school   (school) object &#39;Choate&#39; &#39;Deerfield&#39; ... &quot;St. Paul&#39;s&quot; &#39;Mt. Hermon&#39;</span>
<span class="go">Data variables:</span>
<span class="go">    mu       float64 1.0</span>
<span class="go">    theta_t  (school) float64 1.0 1.002 1.004 1.0 0.9999 1.0 1.0 1.006</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="pymc3.stats.effective_n">
<code class="sig-prename descclassname">pymc3.stats.</code><code class="sig-name descname">effective_n</code><span class="sig-paren">(</span><em class="sig-param">*args</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#pymc3.stats.effective_n" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculate estimate of the effective sample size.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>data</strong><span class="classifier">obj</span></dt><dd><p>Any object that can be converted to an az.InferenceData object.
Refer to documentation of az.convert_to_dataset for details.
For ndarray: shape = (chain, draw).
For n-dimensional ndarray transform first to dataset with az.convert_to_dataset.</p>
</dd>
<dt><strong>var_names</strong><span class="classifier">list</span></dt><dd><p>Names of variables to include in the effective_sample_size_mean report</p>
</dd>
<dt><strong>method</strong><span class="classifier">str</span></dt><dd><p>Select ess method. Valid methods are:</p>
<ul class="simple">
<li><p>“bulk”</p></li>
<li><p>“tail”     # prob, optional</p></li>
<li><p>“quantile” # prob</p></li>
<li><p>“mean” (old ess)</p></li>
<li><p>“sd”</p></li>
<li><p>“median”</p></li>
<li><p>“mad” (mean absolute deviance)</p></li>
<li><p>“z_scale”</p></li>
<li><p>“folded”</p></li>
<li><p>“identity”</p></li>
</ul>
</dd>
<dt><strong>relative</strong><span class="classifier">bool</span></dt><dd><p>Return relative ess
<cite>ress = ess / n</cite></p>
</dd>
<dt><strong>prob</strong><span class="classifier">float, optional</span></dt><dd><p>probability value for “tail” and “quantile” ess functions.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt>xarray.Dataset</dt><dd><p>Return the effective sample size, <span class="math notranslate nohighlight">\(\hat{N}_{eff}\)</span></p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>The basic ess diagnostic is computed by:
.. math:: hat{N}_{eff} = frac{MN}{hat{tau}}
.. math:: hat{tau} = -1 + 2 sum_{t’=0}^K hat{P}_{t’}</p>
<p>where <span class="math notranslate nohighlight">\(M\)</span> is the number of chains, <span class="math notranslate nohighlight">\(N\)</span> the number of draws,
<span class="math notranslate nohighlight">\(\hat{\rho}_t\)</span> is the estimated _autocorrelation at lag <span class="math notranslate nohighlight">\(t\)</span>, and
<span class="math notranslate nohighlight">\(K\)</span> is the last integer for which <span class="math notranslate nohighlight">\(\hat{P}_{K} = \hat{\rho}_{2K} +
\hat{\rho}_{2K+1}\)</span> is still positive.</p>
<p>The current implementation is similar to Stan, which uses Geyer’s initial monotone sequence
criterion (Geyer, 1992; Geyer, 2011).</p>
<p class="rubric">References</p>
<ul class="simple">
<li><p>Vehtari et al. (2019) see <a class="reference external" href="https://arxiv.org/abs/1903.08008">https://arxiv.org/abs/1903.08008</a></p></li>
<li><p><a class="reference external" href="https://mc-stan.org/docs/2_18/reference-manual/effective-sample-size-section.html">https://mc-stan.org/docs/2_18/reference-manual/effective-sample-size-section.html</a>
Section 15.4.2</p></li>
<li><p>Gelman et al. BDA (2014) Formula 11.8</p></li>
</ul>
<p class="rubric">Examples</p>
<p>Calculate the effective_sample_size using the default arguments:</p>
<div class="highlight-ipython notranslate"><div class="highlight"><pre><span></span><span class="gp">In [1]: </span><span class="kn">import</span> <span class="nn">arviz</span> <span class="k">as</span> <span class="nn">az</span>
<span class="gp">   ...: </span><span class="n">data</span> <span class="o">=</span> <span class="n">az</span><span class="o">.</span><span class="n">load_arviz_data</span><span class="p">(</span><span class="s1">&#39;non_centered_eight&#39;</span><span class="p">)</span>
<span class="gp">   ...: </span><span class="n">az</span><span class="o">.</span><span class="n">ess</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
<span class="gp">   ...: </span>
<span class="gh">Out[1]: </span><span class="go"></span>
<span class="go">&lt;xarray.Dataset&gt;</span>
<span class="go">Dimensions:  (school: 8)</span>
<span class="go">Coordinates:</span>
<span class="go">  * school   (school) object &#39;Choate&#39; &#39;Deerfield&#39; ... &quot;St. Paul&#39;s&quot; &#39;Mt. Hermon&#39;</span>
<span class="go">Data variables:</span>
<span class="go">    mu       float64 2.353e+03</span>
<span class="go">    theta_t  (school) float64 2.215e+03 3.157e+03 ... 2.679e+03 2.522e+03</span>
<span class="go">    tau      float64 1.268e+03</span>
<span class="go">    theta    (school) float64 2.298e+03 2.432e+03 ... 2.173e+03 2.277e+03</span>
</pre></div>
</div>
<p>Calculate the ress of some of the variables</p>
<div class="highlight-ipython notranslate"><div class="highlight"><pre><span></span><span class="gp">In [2]: </span><span class="n">az</span><span class="o">.</span><span class="n">ess</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">relative</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">var_names</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;mu&quot;</span><span class="p">,</span> <span class="s2">&quot;theta_t&quot;</span><span class="p">])</span>
<span class="gh">Out[2]: </span><span class="go"></span>
<span class="go">&lt;xarray.Dataset&gt;</span>
<span class="go">Dimensions:  (school: 8)</span>
<span class="go">Coordinates:</span>
<span class="go">  * school   (school) object &#39;Choate&#39; &#39;Deerfield&#39; ... &quot;St. Paul&#39;s&quot; &#39;Mt. Hermon&#39;</span>
<span class="go">Data variables:</span>
<span class="go">    mu       float64 1.176</span>
<span class="go">    theta_t  (school) float64 1.107 1.579 1.464 1.258 1.157 1.275 1.339 1.261</span>
</pre></div>
</div>
<p>Calculate the ess using the “tail” method, leaving the <cite>prob</cite> argument at its default
value.</p>
<div class="highlight-ipython notranslate"><div class="highlight"><pre><span></span><span class="gp">In [3]: </span><span class="n">az</span><span class="o">.</span><span class="n">ess</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s2">&quot;tail&quot;</span><span class="p">)</span>
<span class="gh">Out[3]: </span><span class="go"></span>
<span class="go">&lt;xarray.Dataset&gt;</span>
<span class="go">Dimensions:  (school: 8)</span>
<span class="go">Coordinates:</span>
<span class="go">  * school   (school) object &#39;Choate&#39; &#39;Deerfield&#39; ... &quot;St. Paul&#39;s&quot; &#39;Mt. Hermon&#39;</span>
<span class="go">Data variables:</span>
<span class="go">    mu       float64 1.401e+03</span>
<span class="go">    theta_t  (school) float64 1.45e+03 1.514e+03 ... 1.207e+03 1.589e+03</span>
<span class="go">    tau      float64 900.0</span>
<span class="go">    theta    (school) float64 1.445e+03 1.506e+03 ... 1.433e+03 1.418e+03</span>
</pre></div>
</div>
</dd></dl>

</div>


    </div>
</div>
<div class="ui vertical footer segment">
    <div class="ui center aligned container">
        <a href="https://github.com/pymc-devs/pymc3"><i class="github icon large"></i></a>
        <a href="https://twitter.com/pymc_devs"><i class="twitter icon large"></i></a>
        <a href="https://discourse.pymc.io/"><i class="discourse icon large"></i></a>
    </div>
    <div class="ui center aligned container">
        <p>
            &copy; Copyright 2018, The PyMC Development Team.
        </p>
        <p>
            Created using <a href="https://sphinx-doc.org/">Sphinx</a> 2.2.1.<br />
        </p>
    </div>
</div>
  </body>
</html>